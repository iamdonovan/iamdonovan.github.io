

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>what is machine learning? &#8212; space cameras and glaciers</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css" />
    <link rel="stylesheet" href="../../../_static/customstyle.css" type="text/css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/sphinx_highlight.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script src="../../../_static/design-tabs.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'teaching/ml-crash-course/exercises/regression';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="training, testing, and validation" href="training_testing.html" />
    <link rel="prev" title="exercises" href="index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">space cameras and glaciers</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../whataboutbob.html">about me</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../../index.html">teaching</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="../../egm101/index.html">EGM101: skills toolbox</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../egm101/lectures.html">lectures</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../egm101/practicals/index.html">practicals</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../egm101/practicals/week5.html">computation, summary statistics, and graphing in excel</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm101/practicals/week6.html">summarizing and graphing data in spss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm101/practicals/week7.html">correlation and regression in spss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm101/practicals/week8.html">hypothesis testing using spss</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../egm101/resources.html">additional resources</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../egm310/index.html">EGM310: introduction to gis and remote sensing</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../egm310/lectures.html">lectures</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../egm310/resources.html">additional resources</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../egm702/index.html">EGM702: photogrammetry and advanced image analysis</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../egm702/lectures.html">lectures</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../egm702/setup/index.html">setup</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../egm702/setup/micmac.html">micmac and cloudcompare</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm702/setup/qgis.html">QGIS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm702/setup/orfeo.html">Orfeo ToolBox for QGIS</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../egm702/practicals/index.html">practicals</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../egm702/practicals/week1.html">dem processing using micmac</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm702/practicals/week2/index.html">dem differencing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm702/practicals/week3/index.html">image transformation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm702/practicals/week4/index.html">change detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm702/practicals/week5/index.html">image classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm702/practicals/gee/index.html">optional gee practicals</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../egm702/resources.html">additional resources</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../egm703/index.html">EGM703: advanced active and passive remote sensing</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../egm703/lectures.html">lectures</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../egm703/practicals/index.html">practicals</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../egm703/practicals/week1.html">urban heat islands</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm703/practicals/week2.html">hyperspectral image analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm703/practicals/week3.html">SAR image processing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm703/practicals/week4.html">InSAR Processing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm703/practicals/week5.html">SAR image interpretation and flood mapping</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../egm703/resources.html">additional resources</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../egm722/index.html">EGM722: programming for gis and remote sensing</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../egm722/setup/index.html">setup</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/setup/github.html">setting up github</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/setup/git.html">installing git</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/setup/desktop.html">installing github desktop</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/setup/fork.html">forking the repository</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/setup/clone.html">cloning the repository</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/setup/conda.html">setting up conda/anaconda</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/setup/environment.html">setting up a conda environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/setup/jupyter.html">configuring jupyter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/setup/pycharm.html">pycharm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/setup/pip.html">installing packages with pip</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../egm722/lectures.html">lectures</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../egm722/practicals/index.html">practicals</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/practicals/pythonintro.html">intro to python</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/practicals/debugging.html">debugging exercise</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/practicals/cartopy.html">mapping with cartopy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/practicals/conflicts.html">conflict resolution (using git)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/practicals/vector.html">vector data using shapely and geopandas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/practicals/folium.html">interactive maps with folium</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/practicals/raster.html">raster data using rasterio</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/practicals/earthaccess.html">searching + downloading satellite data using earthaccess</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/practicals/zonalstats.html">zonal statistics using rasterstats</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/practicals/geopandas.html">more fun with geopandas</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../egm722/faq/index.html">help! something went wrong!</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/faq/git.html">git troubleshooting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/faq/python.html">python troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../egm722/project/index.html">programming project</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/project/github.html">creating a new github repository</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/project/environment.html">environment.yml</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../egm722/project/howto.html">EGM722: A how-to Guide</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../egm722/resources.html">additional resources</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../unis/index.html">unis glaciology (AG-325)</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../unis/optical.html">mapping glacier area change</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../unis/dems.html">dem differencing and geodetic mass balance</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../python-programming/index.html">programming skills for phd researchers (using python)</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../python-programming/setup/index.html">setup</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../python-programming/setup/github.html">setting up github</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../python-programming/setup/git.html">installing git</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../python-programming/setup/desktop.html">installing github desktop</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../python-programming/setup/fork.html">forking the repository</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../python-programming/setup/clone.html">cloning the repository</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../python-programming/setup/conda.html">setting up conda/anaconda</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../python-programming/setup/environment.html">setting up a conda environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../python-programming/setup/jupyter.html">configuring the jupyterlab terminal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../python-programming/setup/pycharm.html">pycharm</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../python-programming/lectures.html">lectures</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../python-programming/exercises/index.html">exercises</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../python-programming/exercises/git.html">introduction to version control using git</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../python-programming/exercises/project.html">starting a new project</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../python-programming/exercises/intro.html">introduction to python using jupyterlab</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../python-programming/exercises/debugging.html">debugging code</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../python-programming/exercises/plotting.html">plotting data using seaborn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../python-programming/exercises/pandas.html">working with pandas dataframes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../python-programming/exercises/stats.html">basic statistical analysis using python</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../python-programming/exercises/regression.html">linear regression using python</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../python-programming/resources.html">additional resources</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../r-programming/index.html">programming skills for phd researchers (using R)</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../r-programming/setup/index.html">setup</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../r-programming/setup/github.html">setting up github</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../r-programming/setup/git.html">installing git</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../r-programming/setup/desktop.html">installing github desktop</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../r-programming/setup/fork.html">forking the repository</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../r-programming/setup/clone.html">cloning the repository</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../r-programming/setup/conda.html">setting up conda/anaconda</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../r-programming/setup/environment.html">setting up a conda environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../r-programming/setup/jupyter.html">configuring jupyterlab</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../r-programming/lectures.html">lectures</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../r-programming/exercises/index.html">exercises</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../r-programming/exercises/git.html">introduction to version control using git</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../r-programming/exercises/project.html">starting a new project</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../r-programming/exercises/rintro.html">introduction to <strong>R</strong> using jupyterlab</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../r-programming/exercises/debugging.html">debugging code</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../r-programming/exercises/plotting.html">plotting data using ggplot2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../r-programming/exercises/transform.html">transforming data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../r-programming/exercises/stats.html">basic statistical analysis using <strong>R</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../r-programming/exercises/regression.html">linear regression using <strong>R</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../r-programming/resources.html">additional resources</a></li>
</ul>
</li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="../index.html">a crash course in machine learning (using python) – under development</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../setup.html">setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lectures.html">lectures</a></li>
<li class="toctree-l3 current active has-children"><a class="reference internal" href="index.html">exercises</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l4 current active"><a class="current reference internal" href="#">what is machine learning?</a></li>
<li class="toctree-l4"><a class="reference internal" href="training_testing.html">training, testing, and validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="feature_design.html">feature design and engineering</a></li>
<li class="toctree-l4"><a class="reference internal" href="performance.html">measuring model performance</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../resources.html">additional resources</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../hpc-intro/index.html">introduction to high performance computing</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-24"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../hpc-intro/setup.html">setup</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../hpc-intro/shell/index.html">the unix shell</a><input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-25"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../hpc-intro/shell/shell.html">in the beginning was the command line</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../hpc-intro/shell/navigation.html">navigating on the command line</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../hpc-intro/shell/files.html">working with files</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../hpc-intro/shell/input.html">input/output</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../hpc-intro/shell/scripts.html">shell scripts</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../hpc-intro/hpc/index.html">high performance computing</a><input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-26"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../hpc-intro/hpc/atlas.html">atlas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../hpc-intro/hpc/data_transfer.html">transferring data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../hpc-intro/hpc/vulcan.html">vulcan</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../hpc-intro/resources.html">additional resources</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dissertation/index.html">undergraduate dissertation projects</a><input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-27"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dissertation/glaciers.html">mapping glacier changes over time</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dissertation/glacial_lakes.html">detection of supraglacial lakes and timing of drainage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dissertation/algae.html">mapping harmful algal blooms using satellite remote sensing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dissertation/floods.html">flood detection and monitoring using remote sensing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dissertation/wildfires.html">detection and mapping of wildfires</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dissertation/heat_islands.html">urban heat islands</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../gee/index.html">google earth engine</a><input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-28"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../gee/tutorials/index.html">gee tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-29"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../gee/tutorials/getting_started/index.html">getting started</a><input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-30"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/getting_started/adding_exporting.html">adding and exporting images</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/getting_started/spectral.html">spectral signatures</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/getting_started/image_collections.html">image collections and vectors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/getting_started/mapping.html">manual mapping (digitizing)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/getting_started/band_maths.html">band maths</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/getting_started/zonal_stats.html">zonal statistics</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../gee/tutorials/classification/index.html">image classification</a><input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-31"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/classification/unsupervised.html">unsupervised classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/classification/pixel.html">pixel-based classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/classification/obia.html">object-based classification</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../gee/tutorials/change_detection/index.html">change detection</a><input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-32"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/change_detection/visual_analysis.html">visual analysis of changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/change_detection/thresholding.html">band maths and thresholding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/change_detection/repeat_classification.html">repeat classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/change_detection/cva.html">change vector analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/change_detection/time_series.html">time series analysis</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../gee/tutorials/enhancement/index.html">image enhancement</a><input class="toctree-checkbox" id="toctree-checkbox-33" name="toctree-checkbox-33" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-33"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/enhancement/filtering.html">image filtering</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/enhancement/pansharpening.html">pansharpening</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/enhancement/edge_detection.html">edge detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/enhancement/unmixing.html">spectral unmixing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/enhancement/pca.html">principle component analysis</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../gee/tutorials/advanced/index.html">advanced topics</a><input class="toctree-checkbox" id="toctree-checkbox-34" name="toctree-checkbox-34" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-34"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../gee/tutorials/advanced/animation.html">creating an animation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../gee/exercises/index.html">sample exercises</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../research/index.html">ongoing and past research projects</a><input class="toctree-checkbox" id="toctree-checkbox-35" name="toctree-checkbox-35" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-35"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../research/dar.html">deplete and retreat (DaR)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../writing/index.html">selected writing samples</a><input class="toctree-checkbox" id="toctree-checkbox-36" name="toctree-checkbox-36" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-36"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../writing/source.html">source magazine (summer 2022)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../github.html">github projects</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/iamdonovan/iamdonovan.github.io" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/iamdonovan/iamdonovan.github.io/edit/main/teaching/ml-crash-course/exercises/regression.rst" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/iamdonovan/iamdonovan.github.io/issues/new?title=Issue%20on%20page%20%2Fteaching/ml-crash-course/exercises/regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/teaching/ml-crash-course/exercises/regression.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>what is machine learning?</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data">data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">linear regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-loss">training and loss</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-and-learning">gradient descent and learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-loss-surface">visualizing the loss surface</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling">scaling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">next steps</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="what-is-machine-learning">
<h1>what is machine learning?<a class="headerlink" href="#what-is-machine-learning" title="Permalink to this heading">#</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a <strong>non-interactive</strong> version of the exercise. If you want to run through the steps yourself and see the
outputs, you’ll need to do one of the following:</p>
<ul class="simple">
<li><p>follow the setup steps and work through the notebook on your own computer</p></li>
<li><p>open the workshop materials on <a class="reference external" href="https://mybinder.org/v2/gh/iamdonovan/ml-crash-course/HEAD">binder</a> and work
through them online</p></li>
</ul>
</div>
<p>In this exercise, we’ll get a look into some of the basics of machine
learning, including:</p>
<ul class="simple">
<li><p>training a model</p></li>
<li><p>loss and loss functions</p></li>
<li><p>learning rates and epochs</p></li>
<li><p>scaling data</p></li>
</ul>
<p>To illustrate these, we’ll use a simple model that we’ve seen plenty of
times before: a linear model with a single variable (feature). Using
machine learning for this sort of problem is largely overkill, but it is
useful to help illustrate some of the points with a familiar example.</p>
<section id="data">
<h2>data<a class="headerlink" href="#data" title="Permalink to this heading">#</a></h2>
<p>The data used in this exercise are the historic meteorological
observations from the <a class="reference external" href="https://www.metoffice.gov.uk/weather/learn-about/how-forecasts-are-made/observations/recording-observations-for-over-100-years">Armagh
Observatory</a>,
downloaded from the <a class="reference external" href="https://www.metoffice.gov.uk/research/climate/maps-and-data/historic-station-data">UK Met
Office</a>.</p>
<p>To make the data slightly easier to work with, I have done the
following: - Removed the header on lines 1-5 - Replaced multiple spaces
with a single space, and replaced single spaces with a comma (<code class="docutils literal notranslate"><span class="pre">,</span></code>) -
Removed <code class="docutils literal notranslate"><span class="pre">---</span></code> to indicate no data, leaving these fields blank -
Removed <code class="docutils literal notranslate"><span class="pre">*</span></code> indicating provisional/estimated values - Removed the 2023
data - Renamed the file <code class="docutils literal notranslate"><span class="pre">armaghdata.csv</span></code>.</p>
<p>If you wish to use your own data (and there are loads of stations
available!), please feel free. ## importing libraries</p>
<p>Before getting started, we will import the libraries (packages) that we
will use in the exercise:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://scikit-learn.org/">sklearn</a>, for fitting a linear model
to our data;</p></li>
<li><p><a class="reference external" href="https://pandas.pydata.org/">pandas</a>, for reading the data from a
file;</p></li>
<li><p><a class="reference external" href="https://numpy.org/">numpy</a>, for working with arrays;</p></li>
<li><p><a class="reference external" href="https://matplotlib.org/">matplotlib</a>, for making plots.</p></li>
</ul>
<p>Remember that to do this, we use the <code class="docutils literal notranslate"><span class="pre">import</span></code> statement, followed by
the name of the package. We can also use <code class="docutils literal notranslate"><span class="pre">from</span></code> to import part of a
package, and we can <em>alias</em> the package name using <code class="docutils literal notranslate"><span class="pre">as</span></code>:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
<p>Next, let’s read in the first dataset that we will use for the exercise,
using <code class="docutils literal notranslate"><span class="pre">pd.read_csv()</span></code>:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/armaghdata.csv&#39;</span><span class="p">)</span> <span class="c1"># read the data</span>
</pre></div>
</div>
<p>For this exercise, we’re going to look at the relationship between
monthly hours of sun and the monthly maximum temperature. Before we
begin with linear regression, we can use the<code class="docutils literal notranslate"><span class="pre">.plot.scatter()</span></code> method
<a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.scatter.html">documentation</a>
with our <strong>DataFrame</strong> to show a scatter plot of these two variables:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s1">&#39;sun&#39;</span><span class="p">,</span> <span class="s1">&#39;tmax&#39;</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../../../_images/regression_5_1.png" src="../../../_images/regression_5_1.png" />
<p>From this plot, we can see that there is an <em>approximately</em> linear
relationship between these two variables - at least, enough so that we
can use it to help us ease into some of the core concepts of machine
learning.</p>
</section>
<section id="linear-regression">
<h2>linear regression<a class="headerlink" href="#linear-regression" title="Permalink to this heading">#</a></h2>
<p>Remember that a linear model with a single variable has the form:</p>
<div class="math notranslate nohighlight">
\[y = \beta + \alpha x\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta\)</span> is the <em>intercept</em> of the line and <span class="math notranslate nohighlight">\(\alpha\)</span> is
the <em>slope</em> of the line.</p>
<p>The terminology often used in machine learning is a little bit
different. The equation for a linear model is often written as:</p>
<div class="math notranslate nohighlight">
\[\hat{y} = b + wx\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{y}\)</span> is the <strong>predicted</strong> value/label, <span class="math notranslate nohighlight">\(b\)</span> is the
<strong>bias</strong>, and <span class="math notranslate nohighlight">\(w\)</span> is the <strong>weight</strong> of the <strong>feature</strong> <span class="math notranslate nohighlight">\(x\)</span>.
Extending this to multiple features (variables), the form looks like:</p>
<div class="math notranslate nohighlight">
\[\hat{y} = b + w_1 x_1 + w_2 x_2 + \cdots + w_n x_n = b + \sum_i w_i x_i\]</div>
<p>Where each feature <span class="math notranslate nohighlight">\(x_i\)</span> has a corresponding weight <span class="math notranslate nohighlight">\(w_i\)</span>.</p>
<p>For this first example, we will look at a model with a single feature
(variable): the relationship between <code class="docutils literal notranslate"><span class="pre">sun</span></code> and <code class="docutils literal notranslate"><span class="pre">tmax</span></code>. To use
<code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> for linear regression, we first create a
<strong>LinearRegression</strong> object
(<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">documentation</a>):</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># instantiate a LinearRegression object</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
</pre></div>
</div>
<p>Next, we can prepare our data. For fitting data with <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>,
we need to make sure that we have dropped all <code class="docutils literal notranslate"><span class="pre">NaN</span></code> values from the
data we want to fit. We also need to reshape each array so that they
have shape <span class="math notranslate nohighlight">\(N\times 1\)</span> (or, for multiple linear regression,
<span class="math notranslate nohighlight">\(N\times m\)</span>, where <span class="math notranslate nohighlight">\(m\)</span> is the number of explanatory
variables we are using for the fit).</p>
<p>To do this, we first use <code class="docutils literal notranslate"><span class="pre">.dropna()</span></code>
(<a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html">documentation</a>)
along with the <code class="docutils literal notranslate"><span class="pre">subset</span></code> argument to remove all <code class="docutils literal notranslate"><span class="pre">NaN</span></code> values from the
<code class="docutils literal notranslate"><span class="pre">sun</span></code> and <code class="docutils literal notranslate"><span class="pre">tmax</span></code> columns.</p>
<p>Then, we use the <code class="docutils literal notranslate"><span class="pre">.to_numpy()</span></code> method
(<a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.to_numpy.html">documentation</a>)
to get the values of each <strong>Series</strong> as an array, before using the
<code class="docutils literal notranslate"><span class="pre">.reshape()</span></code> method
(<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html">documentation</a>)
on the resulting <code class="docutils literal notranslate"><span class="pre">numpy</span></code> <strong>ndarray</strong> to reshape the array so that it
is the correct shape:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># drop all rows where either sun or tmax is NaN</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sun&#39;</span><span class="p">,</span> <span class="s1">&#39;tmax&#39;</span><span class="p">],</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;any&#39;</span><span class="p">)</span>

<span class="n">sun</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;sun&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># reshape so data are N x 1</span>
<span class="n">tmax</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;tmax&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># reshape so data are N x 1</span>
</pre></div>
</div>
<p>Now that we have the data prepared, we can use the <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> method
(<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit">documentation</a>)
of the <strong>LinearRegression</strong> object to calculate the parameters of the
linear model:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">sun</span><span class="p">,</span> <span class="n">tmax</span><span class="p">)</span> <span class="c1"># fit the linear model to our data</span>
</pre></div>
</div>
<p>The value of the weight(s) (slope) is stored in the <code class="docutils literal notranslate"><span class="pre">.coef_</span></code> attribute
of the <strong>LinearRegression</strong> object, and the value of the bias
(intercept) is stored in the <code class="docutils literal notranslate"><span class="pre">.intercept_</span></code> attribute:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span> <span class="c1"># show the coefficient(s) and intercept for the linear model</span>
</pre></div>
</div>
<p>From the above, we can see the weight for the <code class="docutils literal notranslate"><span class="pre">sun</span></code> feature is
0.06515, and the value of the bias is 6.34858. To calculate the
predicted value of the model for new features, we can use the
<code class="docutils literal notranslate"><span class="pre">.predict()</span></code> method
(<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict">documentation</a>).
And, using <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code>, we can show the fitted model alongside the
data:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="c1"># create an array of 5 values from 0 to 300, spaced evenly</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># create a figure with a single axis</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sun</span><span class="p">,</span> <span class="n">tmax</span><span class="p">,</span> <span class="s1">&#39;k.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">)</span> <span class="c1"># plot tmax vs sun as black dots with a label &#39;data&#39;</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="s1">&#39;r--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;linear fit&#39;</span><span class="p">)</span> <span class="c1"># plot the regression line as a red dashed line with label &#39;linear fit&#39;</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span> <span class="c1"># show the legend</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;hours of sun&#39;</span><span class="p">)</span> <span class="c1"># set the xlabel of the axis</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;monthly maximum temperature (°C)&#39;</span><span class="p">)</span> <span class="c1"># set the ylabel of the axis</span>
</pre></div>
</div>
<img alt="../../../_images/regression_15_1.png" src="../../../_images/regression_15_1.png" />
</section>
<section id="training-and-loss">
<h2>training and loss<a class="headerlink" href="#training-and-loss" title="Permalink to this heading">#</a></h2>
<p>In machine learning, the difference between the predicted value/label
and the measured value/label is called <strong>loss</strong>. The goal with machine
learning is to find values for the model <strong>parameters</strong> (e.g., weights
and biases) that minimizes the total or average loss for all examples -
that is, we want the vertical distance between each of our observations
and the regression line to be as small as possible, on average.</p>
<p>We can calculate the loss for our model by first using the
<code class="docutils literal notranslate"><span class="pre">.predict()</span></code> method
(<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict">documentation</a>)
to get the predicted value for each feature value:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># use the fitted parameters to get the predicted values at the input x data</span>
<span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sun</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, we can plot the value of loss for each input feature value, as a
function of the predicted value:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># calculate loss as difference between observed, predicted values</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tmax</span> <span class="o">-</span> <span class="n">predicted</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># create a new figure and axis</span>

<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xmax</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span> <span class="c1"># plot a horizontal line at loss = 0</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">predicted</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span> <span class="c1"># plot the loss as a function of the predicted value</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;predicted value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../../../_images/regression_19_1.png" src="../../../_images/regression_19_1.png" />
<p>From the figure above, we can see that that the loss for our linear
model ranges between -8 and 8, with most of the values falling between
about -4 and 4.</p>
<p>In practice, finding the model parameters that minimize the total or
average loss for all examples means finding the minimum values of a
<strong>cost</strong> or <strong>objective function</strong> (or <strong>loss function</strong>) - that is, a
function that allows us to calculate the total or average loss for all
of our input data.</p>
<p>One commonly used loss function is known as <strong>squared error loss</strong> (or
<strong>L:math:`_2` loss</strong>), which calculates the squared difference between
the label and the predicted value:</p>
<div class="math notranslate nohighlight">
\[(y_i - \hat{y}_i)^2\]</div>
<p>The average loss for all of the input data, or the <strong>mean squared
error</strong> (<strong>MSE</strong>), is calculated as:</p>
<div class="math notranslate nohighlight">
\[MSE = \frac{1}{N} \sum_i (y_i - \hat{y}_i)^2\]</div>
<p>We can calculate and print the value of this loss using the code in the
cell below:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">average_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="c1"># calculate the mean of the squared loss</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;MSE: </span><span class="si">{</span><span class="n">average_loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span> <span class="c1"># print the value of the average loss</span>
</pre></div>
</div>
</section>
<section id="gradient-descent-and-learning">
<h2>gradient descent and learning<a class="headerlink" href="#gradient-descent-and-learning" title="Permalink to this heading">#</a></h2>
<p>The way that we go about finding the minimum value of a function is by
taking the gradient of the function and setting it to zero. The reason
that we use the squared loss, rather than something like the absolute
value, is because the derivatives of the squared loss are easy to
calculate and behave nicely (as opposed to absolute values or cubic or
quartic functions). Depending on the function, we might even be able to
find a <em>closed form</em> solution to the problem - that is, a way to
calculate the optimum parameter values directly. In fact, this is what
has been implemented in <code class="docutils literal notranslate"><span class="pre">LinearRegression.fit()</span></code>: the solution
returned by <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> is found using an approach called Ordinary Least
Squares regression - it’s not actually a machine learning approach at
all.</p>
<p>When we don’t have a nice <em>closed form</em> solution to the minimization
problem, which is very often the case, we use some kind of numerical
optimization method in order to find a solution. One of the more common
approaches that you might come across is something called <strong>gradient
descent</strong>.</p>
<p>Assuming that our cost function is differentiable, we can travel in the
opposite direction of the gradient at a given point (i.e., the slope of
a line or a surface) in order to find a minimum value of the function.</p>
<p>To do this, we first need to take the partial derivatives of the cost
function <span class="math notranslate nohighlight">\(l\)</span> with respect to our model parameters:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial l}{\partial w} = \frac{1}{N} \sum_i -2x_i (y_i - (wx_i + b))\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial l}{\partial b} = \frac{1}{N} \sum_i -2(y_i - (wx_i + b))\]</div>
<p>We then iterate over a number of <strong>epochs</strong> (steps) in order to find a
solution, by calculating the value of the partial derivatives at each
point and subtracting a multiple of this value from the current estimate
of each parameter:</p>
<div class="math notranslate nohighlight">
\[w_{i + 1} = w_i - \alpha \frac{\partial l}{\partial w}(w_i, b_i)\]</div>
<div class="math notranslate nohighlight">
\[b_{i + 1} = b_i - \alpha \frac{\partial l}{\partial b}(w_i, b_i)\]</div>
<p>where the <strong>learning rate</strong> <span class="math notranslate nohighlight">\(\alpha\)</span> controls the amount by which
we update the parameter values at each step. By subtracting the value of
the partial derivative, we ensure that we are always moving toward a 0
value of the partial derivative.</p>
<p>Over the next several blocks of code, we’ll see how we can implement
this before investigating how changing <strong>hyperparameters</strong> like the
learning rate or the number of epochs affects the solution we are able
to find.</p>
<p>First, we’ll define a function, <code class="docutils literal notranslate"><span class="pre">update_parameters()</span></code>, that calculates
the new value of <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span> based on the learning rate and
partial derivatives of the loss function:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">ydata</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">dl_dw</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">xdata</span> <span class="o">*</span> <span class="p">(</span><span class="n">ydata</span> <span class="o">-</span> <span class="p">(</span><span class="n">weight</span> <span class="o">*</span> <span class="n">xdata</span> <span class="o">+</span> <span class="n">bias</span><span class="p">)))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="c1"># calculate the partial derivative of l wrt w</span>
    <span class="n">dl_db</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">ydata</span> <span class="o">-</span> <span class="p">(</span><span class="n">weight</span> <span class="o">*</span> <span class="n">xdata</span> <span class="o">+</span> <span class="n">bias</span><span class="p">)))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="c1"># calculate the partial derivative of l wrt b</span>

    <span class="n">weight</span> <span class="o">-=</span> <span class="n">dl_dw</span> <span class="o">*</span> <span class="n">learning_rate</span> <span class="c1"># subtract dl/dw * learning_rate from w</span>
    <span class="n">bias</span> <span class="o">-=</span> <span class="n">dl_db</span> <span class="o">*</span> <span class="n">learning_rate</span> <span class="c1"># subtract dl/db * learning_rate from b</span>

    <span class="k">return</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span> <span class="c1"># return the updated values of w and b</span>
</pre></div>
</div>
<p>We can also define a function, <code class="docutils literal notranslate"><span class="pre">predict()</span></code>, to help us calculate
<span class="math notranslate nohighlight">\(\hat{y}\)</span> for a given value of <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">xdata</span> <span class="o">+</span> <span class="n">bias</span>
</pre></div>
</div>
<p>along with the loss function, which calculates the mean squared error
for the given values of <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">avg_loss</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">ydata</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">ydata</span> <span class="o">-</span> <span class="n">predict</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="c1"># return the mean of the loss</span>
</pre></div>
</div>
<p>We also want to see how the average loss, weight, and bias values
change, so we can define a function to plot these in a three-panel
figure:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_training_results</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="c1"># create a three panel figure</span>

    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;avg_loss&#39;</span><span class="p">])</span> <span class="c1"># plot avg loss vs epoch in the first panel</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;average loss&#39;</span><span class="p">)</span>

    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;weight&#39;</span><span class="p">])</span> <span class="c1"># plot weight vs epoch in the second panel</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">)</span>

    <span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">])</span> <span class="c1"># plot bias vs epoch in the third panel</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;bias&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axs</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;epoch&#39;</span><span class="p">)</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span> <span class="c1"># ensure that the panels and labels don&#39;t overlap</span>

    <span class="k">return</span> <span class="n">ax</span>
</pre></div>
</div>
<p>And finally, we can write a function, <code class="docutils literal notranslate"><span class="pre">train()</span></code>, to get to the “best”
values of <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p>In this example, I am saving the values to a <strong>DataFrame</strong> every 10
epochs to help cut down on the number of values. At the end, I also
create a plot showing the loss value as a function of the epoch:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">ydata</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">ee</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">ydata</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">ee</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span>
            <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bias</span>
            <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;avg_loss&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">avg_loss</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">ydata</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>

    <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span>
    <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bias</span>
    <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;avg_loss&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">avg_loss</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">ydata</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>

    <span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># plot the value of the average loss for each epoch</span>
    <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
        <span class="n">plot_training_results</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
<p>Now that we have defined the functions that we need in order to train
the model, let’s try this out. Running the cell below will train the
model for 10,000 epochs, with a very small learning rate
(<span class="math notranslate nohighlight">\(5 \times 10^{-8}\)</span>). At the end, we use <code class="docutils literal notranslate"><span class="pre">.tail()</span></code>
(<a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html">documentation</a>)
to view the parameter values and the average loss for the model at the
final step:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># train the model</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">sun</span><span class="p">,</span> <span class="n">tmax</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">6.34</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">5e-8</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>

<span class="n">output</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../../../_images/regression_33_1.png" src="../../../_images/regression_33_1.png" />
<p>We can see that after 10,000 epochs, the parameter values have gotten
close to the optimal values found using <code class="docutils literal notranslate"><span class="pre">LinearRegression.fit()</span></code>,
though we’re not quite there. We can also see that the average loss is
low, but not quite to the level we calculated for those optimum values.
We can also see that there’s a big difference in the range covered by
the calculated values of <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span> - <span class="math notranslate nohighlight">\(w\)</span> ranges from
0 to 0.065, while <span class="math notranslate nohighlight">\(b\)</span> only ranges from 6.34 to 6.340509 - a
difference of only 0.000509.</p>
<p>Try changing the learning rate to a larger value - say, <span class="math notranslate nohighlight">\(10^{-7}\)</span>
- how does this impact the shape of the loss curve? What about the value
for the weight and bias?</p>
<p>With the cell below, we can also see how the regression line changes -
there are large changes in slope for the first 1000 or so epochs, before
the line more or less “settles in” to the values we would expect from
ordinary least squares regression:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sun</span><span class="p">,</span> <span class="n">tmax</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>

<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">351</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">predict</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">],</span> <span class="n">bias</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">]),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;epoch </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span> <span class="s1">&#39;epoch&#39;</span><span class="p">]))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../../../_images/regression_35_1.png" src="../../../_images/regression_35_1.png" />
<p>Now let’s try a slightly larger size - <span class="math notranslate nohighlight">\(7.46 \times 10^{-5}\)</span>.
Here, we see something very interesting with the values of <span class="math notranslate nohighlight">\(w\)</span> and
<span class="math notranslate nohighlight">\(b\)</span> over time:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># train the model with a slightly higher learning rate</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">sun</span><span class="p">,</span> <span class="n">tmax</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mf">6.34</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">7.46e-5</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>

<span class="n">output</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../../../_images/regression_37_1.png" src="../../../_images/regression_37_1.png" />
<p>Rather than starting out low and gradually increasing, the weight starts
out much higher before approaching the “true value”, but from above this
time. Similarly, we see that the bias dips down from 6.3410 before
increasing to over 6.3424 - a much larger range than what we saw with
the smaller time step. Now, try increasing the learning rate from
<span class="math notranslate nohighlight">\(7.46 \times 10^{-5}\)</span> to <span class="math notranslate nohighlight">\(7.5 \times 10^{-5}\)</span> - what
happens?</p>
<p>Now let’s see what happens when we change the starting point - that is,
the initial guess for <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># train the model</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">sun</span><span class="p">,</span> <span class="n">tmax</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>

<span class="n">output</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../../../_images/regression_39_1.png" src="../../../_images/regression_39_1.png" />
<p>From this, we can see that the values of both <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>
are pretty far away from the optimal values - the value for <span class="math notranslate nohighlight">\(b\)</span>
has barely changed from the initial guess, and the value for <span class="math notranslate nohighlight">\(w\)</span>
has converged toward a less than optimal value. Not only that, but the
average loss is much higher than we calculated for the optimal values of
<span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sun</span><span class="p">,</span> <span class="n">tmax</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>

<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">351</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">predict</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">],</span> <span class="n">bias</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">]),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;epoch </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span> <span class="s1">&#39;epoch&#39;</span><span class="p">]))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../../../_images/regression_41_1.png" src="../../../_images/regression_41_1.png" />
</section>
<section id="visualizing-the-loss-surface">
<h2>visualizing the loss surface<a class="headerlink" href="#visualizing-the-loss-surface" title="Permalink to this heading">#</a></h2>
<p>To understand what’s happened here, we’ll look at something called the
<strong>loss surface</strong> - that is, a way to visualize the shape of the loss
function based on different parameter values. To begin, we’ll create a
function to calculate the loss for a range of different parameter
values, then make a contour plot of the loss surface using
<code class="docutils literal notranslate"><span class="pre">.contour()</span></code>
(<a class="reference external" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.contour.html">documentation</a>).
We’ll also make a plot that shows cross-sections of the loss surface:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_loss_surface</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">xdata</span><span class="p">,</span> <span class="n">ydata</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">loss_surf</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">biases</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">weights</span><span class="p">:</span>
            <span class="n">loss_surf</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">avg_loss</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">ydata</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span> <span class="c1">#</span>

    <span class="n">loss_surf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_surf</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">biases</span><span class="p">))</span> <span class="c1"># reshape so that the array is rectangular</span>
    <span class="n">W</span><span class="p">,</span> <span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">)</span> <span class="c1"># get a grid of weight and bias values</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">loss_surf</span><span class="p">)</span> <span class="c1"># plot contours of the loss surface</span>

    <span class="k">return</span> <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span>


<span class="k">def</span> <span class="nf">plot_cross_sections</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">biases</span><span class="p">,</span> <span class="n">xdata</span><span class="p">,</span> <span class="n">ydata</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span> <span class="o">=</span> <span class="n">axs</span>

    <span class="n">losses_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">avg_loss</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">ydata</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">weights</span><span class="p">])</span>
    <span class="n">losses_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">avg_loss</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">ydata</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">biases</span><span class="p">])</span>

    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">losses_w</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">avg_loss</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">ydata</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;ro&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;average loss, $l$&#39;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$w$&#39;</span><span class="p">)</span>

    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">biases</span><span class="p">,</span> <span class="n">losses_b</span><span class="p">,</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">avg_loss</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">ydata</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;ro&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$b$&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span>
</pre></div>
</div>
<p>Now, let’s look at the loss surface for values of <span class="math notranslate nohighlight">\(w\)</span> that range
between -0.2 and 0.2, and values of <span class="math notranslate nohighlight">\(b\)</span> that range between -5 and
25:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plot_loss_surface</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="c1"># 100 values between -0.2 and 0.2</span>
                            <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="c1"># 100 values between -5 and 25</span>
                            <span class="n">sun</span><span class="p">,</span> <span class="n">tmax</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;minimum value&#39;</span><span class="p">)</span>
<span class="c1"># ax.plot(output.weight.values[::10], output.bias.values[::10], &#39;b.&#39;, label=&#39;calculated values&#39;)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;starting point&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$w$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$b$&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../../../_images/regression_45_1.png" src="../../../_images/regression_45_1.png" />
<p>We can see that the surface is fairly lopsided - the range of <span class="math notranslate nohighlight">\(w\)</span>
values is much, much smaller than the range of <span class="math notranslate nohighlight">\(b\)</span> values. Now,
let’s look at a cross section of the surface through our starting point,
holding both <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span> constant:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plot_cross_sections</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">sun</span><span class="p">,</span> <span class="n">tmax</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../../../_images/regression_47_0.png" src="../../../_images/regression_47_0.png" />
<p>Part of the problem here is that there are big differences in the value
of the partial derivatives of <span class="math notranslate nohighlight">\(l\)</span> with respect to <span class="math notranslate nohighlight">\(w\)</span> and
<span class="math notranslate nohighlight">\(b\)</span> are very different. The value of <span class="math notranslate nohighlight">\(\partial l/\partial w\)</span>
at <span class="math notranslate nohighlight">\(w=0\)</span> is -3068.82, while the value of
<span class="math notranslate nohighlight">\(\partial l/\partial b\)</span> at <span class="math notranslate nohighlight">\(b=0\)</span> is -26.28 - this means that
when we multiply by the learning rate, we have a much larger change for
<span class="math notranslate nohighlight">\(w\)</span> compared to <span class="math notranslate nohighlight">\(b\)</span>, and it means that a good learning rate
for one of the parameters is not a good learning rate for the other one
- this is why we see very little change in the value of <span class="math notranslate nohighlight">\(b\)</span>
compared to <span class="math notranslate nohighlight">\(w\)</span>.</p>
<p>As we saw when we increased the learning rate to
<span class="math notranslate nohighlight">\(7.5 \times 10^{-5}\)</span>, large gradient values also mean that when
the learning rate is too large, we end up “jumping” back and forth
across the minimum, and can even end up failing to reach a minimum value
at all.</p>
</section>
<section id="scaling">
<h2>scaling<a class="headerlink" href="#scaling" title="Permalink to this heading">#</a></h2>
<p>One of the ways that we can help counteract this is by <strong>scaling</strong> our
feature and label values. There are a number of ways to do this, but
they typically involve subtracting the mean value and dividing by either
the standard deviation (sometimes also called <strong>standardization</strong>), or
by the range of the dataset (sometimes also called <strong>normalization</strong>):</p>
<div class="math notranslate nohighlight">
\[x_s = \frac{x - \bar{x}}{x_{\rm max} - x_{\rm min}}\]</div>
<p>First, we’ll <em>normalize</em> the values of <code class="docutils literal notranslate"><span class="pre">sun</span></code> and <code class="docutils literal notranslate"><span class="pre">tmax</span></code>:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sun_scaled</span> <span class="o">=</span> <span class="p">(</span><span class="n">sun</span> <span class="o">-</span> <span class="n">sun</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">sun</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">sun</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
<span class="n">tmax_scaled</span> <span class="o">=</span> <span class="p">(</span><span class="n">tmax</span> <span class="o">-</span> <span class="n">tmax</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">tmax</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">tmax</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
</pre></div>
</div>
<p>Now, let’s look at the shape of the loss surface using the scaled
values:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_s</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model_s</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">sun_scaled</span><span class="p">,</span> <span class="n">tmax_scaled</span><span class="p">)</span> <span class="c1"># fit the scaled data</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plot_loss_surface</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="c1"># 100 values between -0.2 and 0.2</span>
                            <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="c1"># 100 values between -5 and 25</span>
                            <span class="n">sun_scaled</span><span class="p">,</span> <span class="n">tmax_scaled</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">model_s</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">model_s</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="s1">&#39;rx&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;minimum value&#39;</span><span class="p">)</span> <span class="c1"># plot the parameters calculated by scaling the data</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;starting point&#39;</span><span class="p">)</span> <span class="c1"># plot the starting point</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$w_s$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$b_s$&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../../../_images/regression_51_1.png" src="../../../_images/regression_51_1.png" />
<p>Notice that the surface is much less lopsided now - the slope of the
surface in the <span class="math notranslate nohighlight">\(b_s\)</span> direction is greater than the slope of the
surface in the <span class="math notranslate nohighlight">\(w_s\)</span> direction, but <span class="math notranslate nohighlight">\(w_s\)</span> and <span class="math notranslate nohighlight">\(b_s\)</span>
are at least the same order of magnitude.</p>
<p>Looking at the cross-section of values, we can see the same:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plot_cross_sections</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">sun_scaled</span><span class="p">,</span> <span class="n">tmax_scaled</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../../../_images/regression_53_0.png" src="../../../_images/regression_53_0.png" />
<p>Using the scaled data, the value of <span class="math notranslate nohighlight">\(\partial l/\partial w\)</span> at
<span class="math notranslate nohighlight">\(w=0\)</span> is -0.064, while the value of <span class="math notranslate nohighlight">\(\partial l/\partial b\)</span>
at <span class="math notranslate nohighlight">\(b=0\)</span> is ~0 (because we have scaled <em>and</em> centered the data).</p>
<p>This has two practical effects: first, it means that we can use much
larger learning rates; and second, we should see that the changes are
more even at each epoch, as opposed to seeing big changes in <span class="math notranslate nohighlight">\(w\)</span>
and almost no change in <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p>In order to get the values of <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span> in the original
units of the data, we need to convert them. Fortunately, we can do this
by re-arranging the following equation so that it is in the form
<span class="math notranslate nohighlight">\(y = wx + b\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac{y - \bar{y}}{y_{\rm max} - y_{\rm min}} = w_s \frac{x - \bar{x}}{x_{\rm max} - x_{\rm min}} + b_s\]</div>
<p>When we do this, we should get the following values for <span class="math notranslate nohighlight">\(w\)</span> and
<span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="math notranslate nohighlight">
\[w = \frac{y_{\rm max} - y_{\rm min}}{x_{\rm max} - x_{\rm min}} w_s\]</div>
<div class="math notranslate nohighlight">
\[b = b_s (y_{\rm max} - y_{\rm min}) + \bar{y} - \frac{y_{\rm max} - y_{\rm min}}{x_{\rm max} - x_{\rm min}} w_s \bar{x}\]</div>
<p>In the function defined in the cell below, we have made a number of
changes. First, we have added the option to scale the data using the
<code class="docutils literal notranslate"><span class="pre">scale</span></code> argument (by default, <code class="docutils literal notranslate"><span class="pre">scale=False</span></code>). If we scale the data,
we make sure to record the un-scaled values of <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>,
using the equations above:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">ydata</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">plot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">scale</span><span class="p">:</span>
        <span class="n">xx</span> <span class="o">=</span> <span class="p">(</span><span class="n">xdata</span> <span class="o">-</span> <span class="n">xdata</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">xdata</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">xdata</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
        <span class="n">yy</span> <span class="o">=</span> <span class="p">(</span><span class="n">ydata</span> <span class="o">-</span> <span class="n">ydata</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">ydata</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">ydata</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">xx</span> <span class="o">=</span> <span class="n">xdata</span>
        <span class="n">yy</span> <span class="o">=</span> <span class="n">ydata</span>

    <span class="k">for</span> <span class="n">ee</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">ee</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">scale</span><span class="p">:</span> <span class="c1"># un-scale the values of weight and bias when we record them to the table</span>
                <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">ydata</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">ydata</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">xdata</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">xdata</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
                <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">*</span> <span class="p">(</span><span class="n">ydata</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">ydata</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">+</span> <span class="n">ydata</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">-</span> <span class="p">(</span><span class="n">ydata</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">ydata</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">xdata</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">xdata</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">*</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">xdata</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span>
                <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bias</span>

            <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;avg_loss&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">avg_loss</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">ydata</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">],</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">scale</span><span class="p">:</span> <span class="c1"># un-scale the values of weight and bias when we record them to the table</span>
        <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span> <span class="o">*</span> <span class="p">(</span><span class="n">ydata</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">ydata</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">xdata</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">xdata</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
        <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">*</span> <span class="p">(</span><span class="n">ydata</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">ydata</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">+</span> <span class="n">ydata</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">-</span> <span class="p">(</span><span class="n">ydata</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">ydata</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">xdata</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">xdata</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">*</span> <span class="n">weight</span> <span class="o">*</span> <span class="n">xdata</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span>
        <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bias</span>

    <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;avg_loss&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">avg_loss</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">ydata</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">],</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ee</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">])</span>
    <span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">plot</span><span class="p">:</span>
        <span class="n">plot_training_results</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
<p>Now, let’s see how well this works by using a learning rate of 0.1, and
training the model for only 1000 epochs:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">sun</span><span class="p">,</span> <span class="n">tmax</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">output</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../../../_images/regression_57_1.png" src="../../../_images/regression_57_1.png" />
<p>Not bad - after only 1000 epochs, we have values for <span class="math notranslate nohighlight">\(w\)</span> and
<span class="math notranslate nohighlight">\(b\)</span> that very nearly match the values calculated using
<code class="docutils literal notranslate"><span class="pre">LinearRegression.fit()</span></code>. Try changing the values of <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>
and <code class="docutils literal notranslate"><span class="pre">epochs</span></code> to see how close to the “true” value you can come. How
large of a learning rate can you have before you fail to get a “good”
solution?</p>
<p>The cell below will plot the regression lines using the parameters from
a few different intermediate epochs, along with the linear fit
calculated using <code class="docutils literal notranslate"><span class="pre">LinearRegression.fit()</span></code> - as before, we should see
that there’s rapid improvement followed by slow improvement, but the end
result is almost indistinguishable from the “true” solution:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sun</span><span class="p">,</span> <span class="n">tmax</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>

<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">351</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]:</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">predict</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span> <span class="s1">&#39;weight&#39;</span><span class="p">],</span> <span class="n">bias</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span> <span class="s1">&#39;bias&#39;</span><span class="p">]),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;epoch </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ind</span><span class="p">,</span> <span class="s1">&#39;epoch&#39;</span><span class="p">]))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ols linear fit&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../../../_images/regression_59_1.png" src="../../../_images/regression_59_1.png" />
</section>
<section id="next-steps">
<h2>next steps<a class="headerlink" href="#next-steps" title="Permalink to this heading">#</a></h2>
<p>That’s all for this exercise. In this exercise, we have seen the basics
of how we can train a model using <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>. We have also used
the example of linear regression to illustrate how training a model
works in the background, by showing how we can tune different
<strong>hyperparameters</strong> to find the optimal model parameters. We have also
seen how important it can be to scale our data when training a model -
it enables us to use larger <strong>learning rate</strong> values and spend less time
training a model; it may also make it possible for the model to converge
to a solution.</p>
<p>In future exercises, we will have a look at some other optimization
techniques - gradient descent is only one technique out of many. There
are also additional machine learning algorithms for which scaling input
data is essential, and we will cover these more as we continue through
the workshop.</p>
<p>For now, try at least one of the following exercises:</p>
<ul class="simple">
<li><p>Train a model for the relationship between <code class="docutils literal notranslate"><span class="pre">tmax</span></code> and <code class="docutils literal notranslate"><span class="pre">tmin</span></code>,
with and without scaling. How large a learning rate are you able to
use, and how many epochs does it take to converge to a good solution?
Does scaling make a large difference to the final result? Why or why
not?</p></li>
<li><p>Train a model for the relationship between <code class="docutils literal notranslate"><span class="pre">sun</span></code> and <code class="docutils literal notranslate"><span class="pre">rain</span></code>,
again with/without scaling. How large a learning rate are you able to
use, and how many epochs does it take to converge to a good solution?
Does scaling make a large difference to the final result? Why or why
not?</p></li>
</ul>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">exercises</p>
      </div>
    </a>
    <a class="right-next"
       href="training_testing.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">training, testing, and validation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data">data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">linear regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-loss">training and loss</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-and-learning">gradient descent and learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-loss-surface">visualizing the loss surface</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling">scaling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">next steps</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Bob McNabb
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2020-2026, Bob McNabb. Licensed under Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0).
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>